# 线性回归
## 案例介绍
本案例将使用加利福利亚房屋数据集，来进行线性回归分析。

该数据集记录了加利福利亚房屋的一些特征以及对应的房屋价格，我们将尝试根据这些特征来预测房屋的价格。

线性回归是一种用于建立变量之间线性关系的预测模型的统计方法，它将自变量和因变量之间的关系表示为一条直线。
## 算法原理
线性回归的目标是找到一条使得预测值与观测值之间的误差最小化的直线。数学上，线性回归模型可以表示为:
```math
y=\beta_0+\beta_1 x_1+\beta_2 x_2+\dots+\beta_n x_n
```
其中， $y$ 是预测的目标量， $\beta_0$ 是截距， $x_1,x_2,\dots,x_n$ 是用于预测 $y$ 的特征变量， $\beta_1,\beta_2,\dots,\beta_n$ 是特征变量的权重。

为了找到最佳拟合直线，我们使用了最小二乘法来最小化实际观测值（真实值）和预测值之间的平方误差，即以下代价函数:
```math
J(\theta)=\dfrac{1}{2m}\sum\limits_{i=1}^{m}{(h_\theta(x^{(i)})-y^{(i)})^2}
```
其中， $\theta$ 是参数向量，$x^{(i)}$ 是第 $i$ 个观测量（训练样本）的输入特征（特征向量）， $y^{(i)}$ 是第 $i$ 个观测量的实际观测值， $m$ 是训练集大小。

最小化代价函数可以通过梯度下降算法来实现。梯度下降通过迭代更新参数向量 $\theta$ ，直到达到代价函数的最小值。梯度下降的迭代更新规则如下
```math
\theta_j=\theta_j-\alpha\dfrac{\partial}{\partial\theta_j}J(\theta)
```
其中， $\alpha$ 是学习率，决定了每次迭代更新的步长
## 数据集
我们将使用加利福利亚房屋数据集，该数据集已经内置在scikit-learn库中。数据集包含506个样本，每个样本有13个特征（如犯罪率、住宅平均房间数等）和一个目标变量（房屋价格）
## 计算步骤
1. 导入必要的库和数据集
2. 数据准备：分割数据集为特征（X）和目标变量（y）
3. 模型训练：使用线性回归模型拟合数据
4. 模型评估：计算拟合直线的性能指标（如均方误差）
5. 结果可视化：绘制拟合直线和实际观测值的散点图
